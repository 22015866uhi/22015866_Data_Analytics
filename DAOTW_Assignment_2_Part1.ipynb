{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22015866uhi/22015866_Data_Analytics/blob/main/DAOTW_Assignment_2_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w9sEQx-NFJv"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this report, we explore the intricate relationship between weather conditions and traffic collisions in New York City. The primary objective is to develop predictive models that can accurately estimate the number of traffic collisions on any given day based on various weather parameters. This study is particularly significant for the emergency services in New York City, as it aims to enable them to optimize their emergency response strategies based on predictive insights.\n",
        "\n",
        "The analysis is divided into two parts: The first part involves constructing a linear regression model, a fundamental yet powerful tool for understanding and predicting relationships between variables. The second part, which will be discussed in a subsequent report, focuses on developing a Deep Neural Network (DNN) regression model, leveraging the advanced capabilities of machine learning.\n",
        "\n",
        "Four distinct datasets have been curated and hosted on GitHub for this analysis. Each dataset incrementally introduces more weather-related variables, starting from basic data on the day of the week and number of collisions, and gradually including temperature, precipitation, and dew point. This progressive approach allows us to understand the impact of each additional weather variable on the accuracy of our predictions.\n",
        "\n",
        "The methodology section that follows will detail the data acquisition process, the preprocessing steps undertaken, the specific approach to linear regression analysis, and the evaluation metrics used to assess the models' performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1 - Linear Regression Model**"
      ],
      "metadata": {
        "id": "ntmNwEL1Bai_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation:**\n",
        "\n",
        "You have imported the data correctly. Ensure that the data is clean, and all the necessary features are included. For predicting the days with the highest number of collisions, the features related to the days of the week and other relevant variables like weather conditions should be included.\n",
        "\n",
        "**Model Building:**\n",
        "\n",
        "From the code cells, it appears you have started building the linear regression model. Make sure to use features that you believe could impact the number of collisions, such as weather conditions and day of the week.\n",
        "\n",
        "**Model Training and Evaluation:**\n",
        "\n",
        "After building the model, you'll need to train it with your training dataset and evaluate its performance using the test dataset.\n",
        "\n",
        "Metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) can be used for evaluation.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "Interpret the results to understand which factors are contributing most to collisions. This can usually be done by examining the coefficients of the linear regression model."
      ],
      "metadata": {
        "id": "s5nWN-RTsrDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Methodology**\n",
        "\n",
        "### **Data Acquisition and Preparation**\n",
        "\n",
        "The initial phase of our analysis involves acquiring and preparing the data for our regression models. This process is fundamental to any data science project, as the quality and structure of the data directly influence the performance and reliability of the models.\n",
        "\n",
        "We now import necessary libraries and loading the datasets. We use pandas, a powerful data manipulation library, and numpy for numerical operations. Four separate datasets (df1, df2, df3, df4) are loaded from a GitHub repository. Each of these datasets represents a different combination of variables:\n",
        "\n",
        "df1: Contains 'day' and 'NUM_COLLISIONS'.\n",
        "df2: Adds 'temp' (temperature) to the variables in df1.\n",
        "df3: Further includes 'dewp' (dew point) to the variables in df2.\n",
        "df4: Adds 'prcp' (precipitation) to the variables in df3."
      ],
      "metadata": {
        "id": "zchoqkWnrzax"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-7cPGENClE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc214072-d233-4ea9-f064-fef784a66926"
      },
      "source": [
        "# needed to create the data frame\n",
        "import pandas as pd\n",
        "\n",
        "# needed to help with speedy maths based calculations\n",
        "import numpy as np\n",
        "\n",
        "# create data frames from csv file we hosted on our github\n",
        "df1 = pd.read_csv('https://raw.githubusercontent.com/22015866uhi/22015866_Data_Analytics/main/linearregressiondata1.csv', index_col=0, )\n",
        "\n",
        "df2 = pd.read_csv('https://raw.githubusercontent.com/22015866uhi/22015866_Data_Analytics/main/linearregressiondata2.csv', index_col=0, )\n",
        "\n",
        "df3 = pd.read_csv('https://raw.githubusercontent.com/22015866uhi/22015866_Data_Analytics/main/linearregressiondata3.csv', index_col=0, )\n",
        "\n",
        "df4 = pd.read_csv('https://raw.githubusercontent.com/22015866uhi/22015866_Data_Analytics/main/linearregressiondata4.csv', index_col=0, )\n",
        "\n",
        "print(df1)\n",
        "print(df2)\n",
        "print(df3)\n",
        "print(df4)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     NUM_COLLISIONS\n",
            "day                \n",
            "4               381\n",
            "5               480\n",
            "6               549\n",
            "7               505\n",
            "2               389\n",
            "..              ...\n",
            "7               448\n",
            "2               355\n",
            "1               384\n",
            "3               518\n",
            "4               443\n",
            "\n",
            "[2535 rows x 1 columns]\n",
            "     temp  NUM_COLLISIONS\n",
            "day                      \n",
            "4    37.8             381\n",
            "5    27.1             480\n",
            "6    28.4             549\n",
            "7    33.4             505\n",
            "2    36.1             389\n",
            "..    ...             ...\n",
            "7    49.4             448\n",
            "2    48.0             355\n",
            "1    42.6             384\n",
            "3    39.4             518\n",
            "4    38.7             443\n",
            "\n",
            "[2535 rows x 2 columns]\n",
            "     temp  dewp  NUM_COLLISIONS\n",
            "day                            \n",
            "4    37.8  23.6             381\n",
            "5    27.1  10.5             480\n",
            "6    28.4  14.1             549\n",
            "7    33.4  18.6             505\n",
            "2    36.1  18.7             389\n",
            "..    ...   ...             ...\n",
            "7    49.4  40.9             448\n",
            "2    48.0  37.4             355\n",
            "1    42.6  30.2             384\n",
            "3    39.4  38.3             518\n",
            "4    38.7  34.9             443\n",
            "\n",
            "[2535 rows x 3 columns]\n",
            "     temp  dewp  prcp  NUM_COLLISIONS\n",
            "day                                  \n",
            "4    37.8  23.6  0.00             381\n",
            "5    27.1  10.5  0.00             480\n",
            "6    28.4  14.1  0.00             549\n",
            "7    33.4  18.6  0.00             505\n",
            "2    36.1  18.7  0.00             389\n",
            "..    ...   ...   ...             ...\n",
            "7    49.4  40.9  0.00             448\n",
            "2    48.0  37.4  0.00             355\n",
            "1    42.6  30.2  0.00             384\n",
            "3    39.4  38.3  0.56             518\n",
            "4    38.7  34.9  0.43             443\n",
            "\n",
            "[2535 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEw-Y4JWNkh1",
        "outputId": "bc635564-66e3-40e0-8c27-86c55b7393ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make sure we have our data by printing it out\n",
        "print(df1[:6])\n",
        "# print(df) #all"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     NUM_COLLISIONS\n",
            "day                \n",
            "4               381\n",
            "5               480\n",
            "6               549\n",
            "7               505\n",
            "2               389\n",
            "1               393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMxwMfGPdWJ"
      },
      "source": [
        "# A scale is not required here, but the constant will be useful\n",
        "SCALE_NUM_COLLISIONS = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now import TensorFlow and Keras, which are essential libraries for building neural network models. Although our current focus is on linear regression, importing these libraries at this stage prepares us for the subsequent Deep Neural Network regression analysis. The TensorFlow version (2.14.0) ensures that we are working with a specific, known version of the library, which is crucial for reproducibility of results:"
      ],
      "metadata": {
        "id": "fQ8nbDL_thMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "5DzJzm8VY1rG",
        "outputId": "8afbcba7-d53b-49e0-f4d8-e520c885f7bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Number of Collisions vs Day**"
      ],
      "metadata": {
        "id": "JovHPmArsu9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We address a crucial aspect of data preparation: restructuring the DataFrame. Since the 'day' variable is set as an index in df1, we reset it to a regular column to facilitate easier manipulation and analysis. We then reconstruct df1 to align the input variables ('day') with the target variable ('NUM_COLLISIONS'). This step is critical for ensuring that our data is in the right format for applying regression models."
      ],
      "metadata": {
        "id": "floNqQHWsjau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index if 'day' is set as index\n",
        "df1.reset_index(inplace=True)\n",
        "\n",
        "# Now, create the DataFrame for input data\n",
        "df1_input_data = [df1[\"day\"], df1[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# Create headers for our new DataFrame. These should correlate with the above.\n",
        "df1_input_headers = [\"day\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# Create a final DataFrame using our new DataFrame and headers.\n",
        "df1 = pd.concat(df1_input_data, axis=1, keys=df1_input_headers)\n",
        "\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "FWRaDMLVs3mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e585d42-2d67-41d1-8082-9470fe7efc61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  NUM_COLLISIONS\n",
            "0       4             381\n",
            "1       5             480\n",
            "2       6             549\n",
            "3       7             505\n",
            "4       2             389\n",
            "...   ...             ...\n",
            "2530    7             448\n",
            "2531    2             355\n",
            "2532    1             384\n",
            "2533    3             518\n",
            "2534    4             443\n",
            "\n",
            "[2535 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Day and NUM_COLLISIONS -  Model Development and Evaluation**\n",
        "\n",
        "Continuing our methodology, we delve into the development of the linear regression model using TensorFlow, a robust framework for building machine learning models.\n",
        "\n",
        "This code block is crucial for dividing our dataset into a training set and a test set. We use an 80-20 split, meaning 80% of the data is used for training the model, and the remaining 20% is reserved for testing its performance. This split is vital for evaluating the model on unseen data, ensuring that our model can generalise well to new data."
      ],
      "metadata": {
        "id": "udP8VoVXzCFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a training set for runnign through the model and a test set, we do this by using sample with 0.8 for an 80% training set and 20% for test.\n",
        "training_set_1 = df1.sample(frac=0.8, random_state=0)\n",
        "test_set_1 = df1.drop(training_set_1.index)"
      ],
      "metadata": {
        "id": "Ty0VniOOvFR0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we prepare our features (inputs) and labels (outputs) for the training and test sets. We separate the 'NUM_COLLISIONS' column from our datasets, which is the target variable we aim to predict. This separation is a standard practice in supervised learning where the model learns to predict the labels from the features."
      ],
      "metadata": {
        "id": "GSOuOOnc1PfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the datasets and remove the final column, i.e. the output column. We do this using pop.\n",
        "training_features_1 = training_set_1.copy()\n",
        "test_features_1 = test_set_1.copy()\n",
        "\n",
        "training_labels_1 = training_features_1.pop('NUM_COLLISIONS')\n",
        "test_labels_1 = test_features_1.pop('NUM_COLLISIONS')"
      ],
      "metadata": {
        "id": "5ojr1YWGvOEj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization of data is addressed in this section. Although in this dataset the labels are already normalized (and thus divided by 1), we include a scale factor (1000) that could be useful for similar datasets with different scales."
      ],
      "metadata": {
        "id": "iky3YLW71d7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I have put in a scale factor and divided by it. In this dataset, I had already normalised and thus it is 1. However, 1000 is what would make sense based on the data here and we can use this later when testing our model..\n",
        "training_labels_1 = training_labels_1/1000\n",
        "test_labels_1 = test_labels_1/1000"
      ],
      "metadata": {
        "id": "rxC88xHsvYTM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We print the training features for inspection and create a normalization layer for our TensorFlow model. This layer is an essential part of preprocessing in neural networks, ensuring that our model inputs have a uniform scale."
      ],
      "metadata": {
        "id": "8GBAdv8A1o4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_features_1)"
      ],
      "metadata": {
        "id": "n_OkqLwdx6X4",
        "outputId": "3ff69581-e973-49f2-c461-f5317b1968d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day\n",
            "828     7\n",
            "2409    4\n",
            "2249    5\n",
            "936     3\n",
            "2390    6\n",
            "...   ...\n",
            "1874    6\n",
            "718     6\n",
            "1149    7\n",
            "387     3\n",
            "1369    4\n",
            "\n",
            "[2028 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization helps in speeding up the training process and improving the performance of the model."
      ],
      "metadata": {
        "id": "9ZNd9ixiEt5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# boiler plate for this model. You can see that we have used the training_features here for our normalisation layer that we try and fit to the outputs.\n",
        "normaliser_1 = tf.keras.layers.Normalization(input_shape=[1,], axis=None) # tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser_1.adapt(np.array(training_features_1))"
      ],
      "metadata": {
        "id": "Qpi6HgPbveSv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define our linear regression model (model_1) using TensorFlow's Sequential API. The model comprises a normalization layer followed by a dense layer with a single unit, which is characteristic of a simple linear regression model. The model is compiled with the Adam optimizer and mean absolute error as the loss function. This setup is fundamental in defining how the model learns during training."
      ],
      "metadata": {
        "id": "43Ljoltb2Q4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have decided to call the model, model_1. We add our normaliser and we are expecting a single output.\n",
        "model_1 = tf.keras.Sequential([\n",
        "    normaliser_1,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "y1-BThwAvhqH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "ddgDNLTfvqOx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are going to fit the model where we require the training features and labels. We will run it 100 times i.e. epochs and we have applied a further 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_1.fit(\n",
        "    training_features_1,\n",
        "    training_labels_1,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "CaJk8EUMvt5s",
        "outputId": "fa4d7f7f-0830-4dac-f589-663c41d784dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13 s, sys: 517 ms, total: 13.5 s\n",
            "Wall time: 13.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will evaluate our model using the test features and labels.\n",
        "mean_absolute_error_model_1 = model_1.evaluate(\n",
        "    test_features_1,\n",
        "    test_labels_1, verbose=0)"
      ],
      "metadata": {
        "id": "g_r2YIo8vzjS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean absolute error of the model can be printed out. Remember, we want to minimise this. Perhaps a model with just day and NUM_COLLISIONS would be better. It will also vary on each training run due to randomisation.\n",
        "print(mean_absolute_error_model_1)"
      ],
      "metadata": {
        "id": "ENe5nS8Qv5vq",
        "outputId": "58bb31b1-d8d0-4fc5-b7cb-ca66150fb8c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06635782867670059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Number of Collisions vs Day and Temp**"
      ],
      "metadata": {
        "id": "d3AiI5v2CrFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We address a crucial aspect of data preparation: restructuring the DataFrame. Since the 'day' variable is set as an index in df1, we reset it to a regular column to facilitate easier manipulation and analysis. We then reconstruct df1 to align the input variables ('day') with the target variable ('NUM_COLLISIONS'). This step is critical for ensuring that our data is in the right format for applying regression models."
      ],
      "metadata": {
        "id": "1bwf1gVOCn3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Collisions vs Day and Temp - Model Development and Evaluation**\n"
      ],
      "metadata": {
        "id": "TS_y_-9uspzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index if 'day' is set as index\n",
        "df2.reset_index(inplace=True)\n",
        "\n",
        "# create a dataframe with the inputs and the output at the end using the imported dataframe. This can be replicated for any configuration, in this case, I have gone for day, temp, wdsp\n",
        "df2_input_data = [df2[\"day\"], df2[\"temp\"], df2[\"NUM_COLLISIONS\"]]\n",
        "# create headers for our new dataframe. These should correlate with the above.\n",
        "df2_input_headers = [\"day\", \"temp\", \"NUM_COLLISIONS\"]\n",
        "# create a final dataframe using our new dataframe and headers.\n",
        "df2 = pd.concat(df2_input_data, axis=1, keys=df2_input_headers)\n",
        "\n",
        "print(df2)"
      ],
      "metadata": {
        "id": "WHuSnr6LdC3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ad26cb-22f7-48d5-c338-be4a20f280fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  NUM_COLLISIONS\n",
            "0       4  37.8             381\n",
            "1       5  27.1             480\n",
            "2       6  28.4             549\n",
            "3       7  33.4             505\n",
            "4       2  36.1             389\n",
            "...   ...   ...             ...\n",
            "2530    7  49.4             448\n",
            "2531    2  48.0             355\n",
            "2532    1  42.6             384\n",
            "2533    3  39.4             518\n",
            "2534    4  38.7             443\n",
            "\n",
            "[2535 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a training set for runnign through the model and a test set, we do this by using sample with 0.8 for an 80% training set and 20% for test.\n",
        "training_set_2 = df2.sample(frac=0.8, random_state=0)\n",
        "test_set_2 = df2.drop(training_set_2.index)"
      ],
      "metadata": {
        "id": "d5JtvdLBcruq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the datasets and remove the final column, i.e. the output column. We do this using pop.\n",
        "training_features_2 = training_set_2.copy()\n",
        "test_features_2 = test_set_2.copy()\n",
        "\n",
        "training_labels_2 = training_features_2.pop('NUM_COLLISIONS')\n",
        "test_labels_2 = test_features_2.pop('NUM_COLLISIONS')"
      ],
      "metadata": {
        "id": "EILvYoDid0wI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I have put in a scale factor and divided by it. In this dataset, I had already normalised and thus it is 1. However, 1000 is what would make sense based on the data here and we can use this later when testing our model..\n",
        "training_labels_2 = training_labels_2/1000\n",
        "test_labels_2 = test_labels_2/1000"
      ],
      "metadata": {
        "id": "3NBSa-Wyf2r6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boiler plate for this model. You can see that we have used the training_features here for our normalisation layer that we try and fit to the outputs.\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features_2))"
      ],
      "metadata": {
        "id": "VQuNfPpdf9al"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have decided to call the model, model_1. We add our normaliser and we are expecting a single output.\n",
        "model_2 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "JKiP5GIrgIJH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more boiler plate for creating a sequential model, we need an optimiser and loss parameter. Here we are going to be using the mean absolute error MAE\n",
        "model_2.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "4odNLaUHgNb7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are going to fit the model where we require the training features and labels. We will run it 100 times i.e. epochs and we have applied a further 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_2.fit(\n",
        "    training_features_2,\n",
        "    training_labels_2,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "rW1K3GxUgQyC",
        "outputId": "6f4aad2c-d03d-4047-c13b-b1e17bc60213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.2 s, sys: 592 ms, total: 14.8 s\n",
            "Wall time: 14.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will evaluate our model using the test features and labels.\n",
        "mean_absolute_error_model_2 = model_2.evaluate(\n",
        "    test_features_2,\n",
        "    test_labels_2, verbose=0)"
      ],
      "metadata": {
        "id": "ALdhLDb_gSjq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean absolute error of the model can be printed out. Remember, we want to minimise this. Perhaps a model with just day and NUM_TRIPS would be better. It will also vary on each training run due to randomisation.\n",
        "print(mean_absolute_error_model_2)"
      ],
      "metadata": {
        "id": "NVLVxc_aghcD",
        "outputId": "3c399040-0b70-4abc-9c14-5b841cc5f601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08275976777076721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try scaling NUM_COLLISIONS\n",
        "training_labels_2 = training_set_2['NUM_COLLISIONS']*SCALE_NUM_COLLISIONS\n",
        "test_labels_2 = test_set_2['NUM_COLLISIONS']*SCALE_NUM_COLLISIONS\n",
        "\n",
        "# Normalization layer for features\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features_2[['day', 'temp']]))\n",
        "\n",
        "# Updated model architecture\n",
        "model_2 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=10, activation='relu'),  # additional layer\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile with a lower learning rate\n",
        "model_2.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model\n",
        "history = model_2.fit(training_features_2, training_labels_2, epochs=100, verbose=1, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOvznOtxKZIB",
        "outputId": "9800c700-b171-4f86-bc25-39536ba25d3d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "51/51 [==============================] - 1s 8ms/step - loss: 0.1549 - val_loss: 0.0716\n",
            "Epoch 2/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0678 - val_loss: 0.0713\n",
            "Epoch 3/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0662 - val_loss: 0.0711\n",
            "Epoch 4/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0649 - val_loss: 0.0633\n",
            "Epoch 5/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0652\n",
            "Epoch 6/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0652\n",
            "Epoch 7/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0706\n",
            "Epoch 8/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0626\n",
            "Epoch 9/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0695\n",
            "Epoch 10/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0629 - val_loss: 0.0655\n",
            "Epoch 11/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.0681\n",
            "Epoch 12/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.0679\n",
            "Epoch 13/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.0622\n",
            "Epoch 14/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.0633\n",
            "Epoch 15/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.0624\n",
            "Epoch 16/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0668\n",
            "Epoch 17/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0651\n",
            "Epoch 18/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.0672\n",
            "Epoch 19/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.0633\n",
            "Epoch 20/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0633\n",
            "Epoch 21/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0631\n",
            "Epoch 22/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0615\n",
            "Epoch 23/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0626\n",
            "Epoch 24/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.0614\n",
            "Epoch 25/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.0634\n",
            "Epoch 26/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0626\n",
            "Epoch 27/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0620\n",
            "Epoch 28/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0627\n",
            "Epoch 29/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0625\n",
            "Epoch 30/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0660\n",
            "Epoch 31/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0615 - val_loss: 0.0630\n",
            "Epoch 32/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0631\n",
            "Epoch 33/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0671\n",
            "Epoch 34/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0631\n",
            "Epoch 35/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.0638\n",
            "Epoch 36/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0619 - val_loss: 0.0634\n",
            "Epoch 37/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.0662\n",
            "Epoch 38/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0616\n",
            "Epoch 39/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0609\n",
            "Epoch 40/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0617\n",
            "Epoch 41/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.0618\n",
            "Epoch 42/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0628\n",
            "Epoch 43/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0617\n",
            "Epoch 44/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0622\n",
            "Epoch 45/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0647\n",
            "Epoch 46/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0598 - val_loss: 0.0626\n",
            "Epoch 47/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0593 - val_loss: 0.0619\n",
            "Epoch 48/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0616\n",
            "Epoch 49/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0626\n",
            "Epoch 50/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.0617\n",
            "Epoch 51/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.0616\n",
            "Epoch 52/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0614\n",
            "Epoch 53/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0630\n",
            "Epoch 54/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.0626\n",
            "Epoch 55/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0614\n",
            "Epoch 56/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.0620\n",
            "Epoch 57/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0667\n",
            "Epoch 58/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0615\n",
            "Epoch 59/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0627\n",
            "Epoch 60/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0619\n",
            "Epoch 61/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0614\n",
            "Epoch 62/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0626\n",
            "Epoch 63/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.0640\n",
            "Epoch 64/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0637\n",
            "Epoch 65/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0611\n",
            "Epoch 66/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0636\n",
            "Epoch 67/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.0612\n",
            "Epoch 68/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.0613\n",
            "Epoch 69/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0628\n",
            "Epoch 70/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0627\n",
            "Epoch 71/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0604 - val_loss: 0.0616\n",
            "Epoch 72/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0646\n",
            "Epoch 73/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0611\n",
            "Epoch 74/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0603 - val_loss: 0.0637\n",
            "Epoch 75/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0608 - val_loss: 0.0614\n",
            "Epoch 76/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0604 - val_loss: 0.0629\n",
            "Epoch 77/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0604 - val_loss: 0.0631\n",
            "Epoch 78/100\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0673\n",
            "Epoch 79/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0606 - val_loss: 0.0623\n",
            "Epoch 80/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0613\n",
            "Epoch 81/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0624\n",
            "Epoch 82/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0631\n",
            "Epoch 83/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0610\n",
            "Epoch 84/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0598 - val_loss: 0.0636\n",
            "Epoch 85/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0617\n",
            "Epoch 86/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0632\n",
            "Epoch 87/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0643\n",
            "Epoch 88/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0638\n",
            "Epoch 89/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0600 - val_loss: 0.0618\n",
            "Epoch 90/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0630\n",
            "Epoch 91/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0605 - val_loss: 0.0615\n",
            "Epoch 92/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0618\n",
            "Epoch 93/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0614\n",
            "Epoch 94/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0603 - val_loss: 0.0627\n",
            "Epoch 95/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0633\n",
            "Epoch 96/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0633\n",
            "Epoch 97/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0615\n",
            "Epoch 98/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0629\n",
            "Epoch 99/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0624\n",
            "Epoch 100/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Number of Collisions vs Day, Temp and Dewp**"
      ],
      "metadata": {
        "id": "KtAHTS6YYGFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index if 'day' is set as index\n",
        "df3.reset_index(inplace=True)\n",
        "\n",
        "# create a dataframe with the inputs and the output at the end using the imported dataframe. This can be replicated for any configuration, in this case, I have gone for day, temp, wdsp\n",
        "df3_input_data = [df3[\"day\"], df3[\"temp\"], df3[\"dewp\"], df3[\"NUM_COLLISIONS\"]]\n",
        "# create headers for our new dataframe. These should correlate with the above.\n",
        "df3_input_headers = [\"day\", \"temp\", \"dewp\", \"NUM_COLLISIONS\"]\n",
        "# create a final dataframe using our new dataframe and headers.\n",
        "df3 = pd.concat(df3_input_data, axis=1, keys=df3_input_headers)\n",
        "\n",
        "print(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zzk0vLtt61X",
        "outputId": "8ff9d30b-9c04-442a-fa08-e52e624ef1db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  NUM_COLLISIONS\n",
            "0       4  37.8  23.6             381\n",
            "1       5  27.1  10.5             480\n",
            "2       6  28.4  14.1             549\n",
            "3       7  33.4  18.6             505\n",
            "4       2  36.1  18.7             389\n",
            "...   ...   ...   ...             ...\n",
            "2530    7  49.4  40.9             448\n",
            "2531    2  48.0  37.4             355\n",
            "2532    1  42.6  30.2             384\n",
            "2533    3  39.4  38.3             518\n",
            "2534    4  38.7  34.9             443\n",
            "\n",
            "[2535 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a training set for runnign through the model and a test set, we do this by using sample with 0.8 for an 80% training set and 20% for test.\n",
        "training_set_3 = df3.sample(frac=0.8, random_state=0)\n",
        "test_set_3 = df3.drop(training_set_2.index)"
      ],
      "metadata": {
        "id": "2nOARsmkwiQ5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the datasets and remove the final column, i.e. the output column. We do this using pop.\n",
        "training_features_3 = training_set_3.copy()\n",
        "test_features_3 = test_set_3.copy()\n",
        "\n",
        "# Try without scaling NUM_COLLISIONS\n",
        "training_labels_3 = training_set_3['NUM_COLLISIONS']\n",
        "test_labels_3 = test_set_3['NUM_COLLISIONS']"
      ],
      "metadata": {
        "id": "F3J7Yb-0xT0l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I have put in a scale factor and divided by it. In this dataset, I had already normalised and thus it is 1. However, 1000 is what would make sense based on the data here and we can use this later when testing our model..\n",
        "training_labels_3 = training_labels_3/1000\n",
        "test_labels_3 = test_labels_3/1000"
      ],
      "metadata": {
        "id": "9CKzeMl90dd2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of training_features_3 to check its structure\n",
        "print(training_features_3.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SGOKzHQ6E5h",
        "outputId": "ecd0032f-a810-431f-b58d-e8eb6ca2740c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  NUM_COLLISIONS\n",
            "828     7  60.5  48.6             679\n",
            "2409    4  67.7  56.8             529\n",
            "2249    5  44.3  24.1             584\n",
            "936     3  81.3  60.0             705\n",
            "2390    6  74.1  66.5             646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize features\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(training_features_3[['day', 'temp', 'dewp']])\n"
      ],
      "metadata": {
        "id": "0hbYWExe6VAm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "model_3 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=10, activation='relu'),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_3.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "id": "dAiYCQwI6em4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model_3.fit(\n",
        "    training_features_3[['day', 'temp', 'dewp']],\n",
        "    training_labels_3,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40URDQ3o6ist",
        "outputId": "8428bdcd-d49e-4ac9-c4d2-0db175b6e137"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "51/51 [==============================] - 1s 7ms/step - loss: 0.1552 - val_loss: 0.0872\n",
            "Epoch 2/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0741 - val_loss: 0.0675\n",
            "Epoch 3/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.0647\n",
            "Epoch 4/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.0651\n",
            "Epoch 5/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0647 - val_loss: 0.0644\n",
            "Epoch 6/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.0638\n",
            "Epoch 7/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.0664\n",
            "Epoch 8/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0646 - val_loss: 0.0649\n",
            "Epoch 9/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0641 - val_loss: 0.0693\n",
            "Epoch 10/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0628\n",
            "Epoch 11/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0622\n",
            "Epoch 12/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0608 - val_loss: 0.0666\n",
            "Epoch 13/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.0671\n",
            "Epoch 14/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.0667\n",
            "Epoch 15/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.0632\n",
            "Epoch 16/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0640\n",
            "Epoch 17/100\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0613 - val_loss: 0.0624\n",
            "Epoch 18/100\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0610 - val_loss: 0.0639\n",
            "Epoch 19/100\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0611 - val_loss: 0.0633\n",
            "Epoch 20/100\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0623 - val_loss: 0.0656\n",
            "Epoch 21/100\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0629 - val_loss: 0.0623\n",
            "Epoch 22/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0629\n",
            "Epoch 23/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0627\n",
            "Epoch 24/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0613 - val_loss: 0.0661\n",
            "Epoch 25/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0644\n",
            "Epoch 26/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0617\n",
            "Epoch 27/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0634\n",
            "Epoch 28/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0626 - val_loss: 0.0629\n",
            "Epoch 29/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0614 - val_loss: 0.0630\n",
            "Epoch 30/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0621\n",
            "Epoch 31/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0608 - val_loss: 0.0629\n",
            "Epoch 32/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0671\n",
            "Epoch 33/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0623\n",
            "Epoch 34/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.0658\n",
            "Epoch 35/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.0635\n",
            "Epoch 36/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0713\n",
            "Epoch 37/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.0625\n",
            "Epoch 38/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0618\n",
            "Epoch 39/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0632\n",
            "Epoch 40/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0602 - val_loss: 0.0620\n",
            "Epoch 41/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0604 - val_loss: 0.0637\n",
            "Epoch 42/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0608 - val_loss: 0.0655\n",
            "Epoch 43/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0642\n",
            "Epoch 44/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0622\n",
            "Epoch 45/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0614 - val_loss: 0.0628\n",
            "Epoch 46/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0608 - val_loss: 0.0648\n",
            "Epoch 47/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0634\n",
            "Epoch 48/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0619 - val_loss: 0.0635\n",
            "Epoch 49/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0644\n",
            "Epoch 50/100\n",
            "51/51 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 0.0621\n",
            "Epoch 51/100\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.0605 - val_loss: 0.0643\n",
            "Epoch 52/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0602 - val_loss: 0.0648\n",
            "Epoch 53/100\n",
            "51/51 [==============================] - 1s 20ms/step - loss: 0.0609 - val_loss: 0.0639\n",
            "Epoch 54/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0629\n",
            "Epoch 55/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0636\n",
            "Epoch 56/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0630\n",
            "Epoch 57/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0627\n",
            "Epoch 58/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0611 - val_loss: 0.0623\n",
            "Epoch 59/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0659\n",
            "Epoch 60/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0624 - val_loss: 0.0628\n",
            "Epoch 61/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0654\n",
            "Epoch 62/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0638\n",
            "Epoch 63/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0631\n",
            "Epoch 64/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0639\n",
            "Epoch 65/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0626\n",
            "Epoch 66/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0626\n",
            "Epoch 67/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.0619\n",
            "Epoch 68/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0617\n",
            "Epoch 69/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.0642\n",
            "Epoch 70/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.0656\n",
            "Epoch 71/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0629\n",
            "Epoch 72/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0604 - val_loss: 0.0637\n",
            "Epoch 73/100\n",
            "51/51 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0620\n",
            "Epoch 74/100\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.0600 - val_loss: 0.0632\n",
            "Epoch 75/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.0621\n",
            "Epoch 76/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0619 - val_loss: 0.0623\n",
            "Epoch 77/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0670\n",
            "Epoch 78/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0610 - val_loss: 0.0626\n",
            "Epoch 79/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0628\n",
            "Epoch 80/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0611 - val_loss: 0.0618\n",
            "Epoch 81/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0637\n",
            "Epoch 82/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0633\n",
            "Epoch 83/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.0640\n",
            "Epoch 84/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0621\n",
            "Epoch 85/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0621\n",
            "Epoch 86/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0709\n",
            "Epoch 87/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.0625\n",
            "Epoch 88/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.0667\n",
            "Epoch 89/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0638\n",
            "Epoch 90/100\n",
            "51/51 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.0656\n",
            "Epoch 91/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.0621\n",
            "Epoch 92/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0624\n",
            "Epoch 93/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0634\n",
            "Epoch 94/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0633\n",
            "Epoch 95/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0632\n",
            "Epoch 96/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.0703\n",
            "Epoch 97/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0626\n",
            "Epoch 98/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.0662\n",
            "Epoch 99/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.0629\n",
            "Epoch 100/100\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.0633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "_MVNxHslA2fG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Collisions Predictions vs Days**"
      ],
      "metadata": {
        "id": "3p8PCL4ZqbUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iloc allows us to select by rows. Here, we are shuffling the data by rows determined at random.\n",
        "shuffle = df3.iloc[np.random.permutation(len(df3))]\n",
        "\n",
        "# we are selecting all rows of the columns outliined i.e. The 3rd (2 as indexes start from 0)\n",
        "predictors = shuffle.iloc[:,0:3]\n",
        "# Since it is the last column, we can also use\n",
        "# predictorTest = shuffle.iloc[:,-1]\n",
        "\n",
        "# print out the first 6 rows of predictors.\n",
        "print(predictors[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxTafyu4btTs",
        "outputId": "e75f3e91-c10e-43cc-bd92-874c2c19b826"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp\n",
            "1371    6  60.1  54.1\n",
            "1947    1  67.5  60.9\n",
            "945     5  77.0  59.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we create a custom dataframe with 3 values per feature.\n",
        "input_1 = pd.DataFrame.from_dict(data = {'day' : [6,1,5]})\n",
        "\n",
        "# next we can check this out, you can multiply by 1000 to get more realistic NUM_COLLISIONS values.\n",
        "linear_day_predictions = model_1.predict(input_1[:3])*1000 # essentially 1000 in this instance would give back realistic numbers based on the NUM_COLLISIONS data\n",
        "print(linear_day_predictions)"
      ],
      "metadata": {
        "id": "bk96l6IfjA_1",
        "outputId": "c07a1afc-0cb1-412b-ba95-06003f8b7d3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "[[642.8704 ]\n",
            " [538.7159 ]\n",
            " [622.03955]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for prediction\n",
        "# Assuming you want to predict for all days, create a DataFrame for this\n",
        "days_data = pd.DataFrame({'day': range(1, 8)})  # 1 to 7 representing days of the week\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_1.predict(days_data)\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "days_data['Predicted_Collisions'] = predictions.flatten()\n",
        "\n",
        "# Assuming the scale factor used was SCALE_NUM_COLLISIONS\n",
        "SCALE_NUM_COLLISIONS = 0.001  # Replace with your actual scale factor\n",
        "days_data['Predicted_Collisions'] = days_data['Predicted_Collisions'] / SCALE_NUM_COLLISIONS\n",
        "\n",
        "# Analyze which days have higher predicted collisions\n",
        "print(days_data.sort_values(by='Predicted_Collisions', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2auDzZg0x6be",
        "outputId": "743a3abf-01f5-4cb6-d585-78814099b0ec"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "   day  Predicted_Collisions\n",
            "6    7            663.701294\n",
            "5    6            642.870422\n",
            "4    5            622.039551\n",
            "3    4            601.208618\n",
            "2    3            580.377747\n",
            "1    2            559.546814\n",
            "0    1            538.715881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions and Insights**\n",
        "\n",
        "**Day-wise Collision Risk:** Based on these predictions, it appears that the model estimates a higher number of collisions later in the week, with the highest predicted collisions on day 7 (which might correspond to Sunday, depending on how you've coded the days).\n",
        "\n",
        "**Actionable Insights:** Emergency services could use this information to prepare more resources or be on higher alert towards the end of the week, especially on days 6 and 7."
      ],
      "metadata": {
        "id": "HxjRyd5o0t1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Collisions Predictions vs Days and Temp**"
      ],
      "metadata": {
        "id": "mb8W4ZJwrJo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Input\n",
        "input_2 = pd.DataFrame.from_dict({'day': [6, 1, 5], 'temp': [60.1, 67.5, 77.0]})\n",
        "\n",
        "# Predictions\n",
        "linear_day_temp_predictions = model_2.predict(input_2)*1000\n",
        "print(linear_day_temp_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P9Q-22G96a7",
        "outputId": "06f8ddf5-c2a2-4f70-d70a-b5611c0a328f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 42ms/step\n",
            "[[634.70325]\n",
            " [505.4407 ]\n",
            " [641.42535]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use df2 for predictions\n",
        "predictions_2 = model_2.predict(df2[['day', 'temp']])\n",
        "\n",
        "# Add predictions to df2 for analysis\n",
        "df2['Predicted_Collisions'] = predictions_2.flatten()\n",
        "\n",
        "# Assuming the scale factor used was SCALE_NUM_COLLISIONS\n",
        "SCALE_NUM_COLLISIONS = 0.001  # Replace with your actual scale factor\n",
        "df2['Predicted_Collisions'] = df2['Predicted_Collisions'] / SCALE_NUM_COLLISIONS\n",
        "\n",
        "# Analyze the predictions\n",
        "print(df2.sort_values(by='Predicted_Collisions', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UikcAi4-66kE",
        "outputId": "ede0609e-2dd5-411b-83ec-b9e475ed87d2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 0s 2ms/step\n",
            "      day  temp  NUM_COLLISIONS  Predicted_Collisions\n",
            "198     7  89.1             697            707.153442\n",
            "1338    7  85.0             715            700.081177\n",
            "1282    7  83.0             798            696.631287\n",
            "149     7  83.0             736            696.631287\n",
            "1289    7  82.8             769            696.286316\n",
            "...   ...   ...             ...                   ...\n",
            "1456    1  20.1             406            453.098236\n",
            "1822    1  19.2             494            452.481415\n",
            "768     1  18.9             454            452.275848\n",
            "1815    1   9.7             475            445.970825\n",
            "1131    1   6.9             309            444.051849\n",
            "\n",
            "[2535 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Collisions Predictions vs Days, Temp and Dewp**"
      ],
      "metadata": {
        "id": "Md2ejsqr9iyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction input\n",
        "input_3 = pd.DataFrame({'day': [6, 1, 5], 'temp': [60.1, 67.5, 77.0], 'dewp': [54.1, 60.9, 59.7]})\n",
        "\n",
        "# Predictions\n",
        "linear_day_temp_dewp_predictions = model_3.predict(input_3)*1000\n",
        "print(linear_day_temp_dewp_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xfh4Sig6y5C",
        "outputId": "b411a5d9-9a32-4a93-ddb5-ce584196437b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 69ms/step\n",
            "[[662.01465]\n",
            " [535.937  ]\n",
            " [672.81256]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use df3 for predictions\n",
        "predictions_3 = model_3.predict(df3[['day', 'temp', 'dewp']])\n",
        "\n",
        "# Add predictions to df3 for analysis\n",
        "df3['Predicted_Collisions'] = predictions_3.flatten()\n",
        "\n",
        "# Assuming the scale factor used was SCALE_NUM_COLLISIONS\n",
        "SCALE_NUM_COLLISIONS = 0.001  # Replace with your actual scale factor\n",
        "df3['Predicted_Collisions'] = df3['Predicted_Collisions'] / SCALE_NUM_COLLISIONS\n",
        "\n",
        "# Analyze the predictions\n",
        "print(df3.sort_values(by='Predicted_Collisions', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-W5uF5j0rzZ",
        "outputId": "31fd771e-dfd9-4cd1-8159-a62eab058f3d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 0s 4ms/step\n",
            "      day  temp  dewp  NUM_COLLISIONS  Predicted_Collisions\n",
            "198     7  89.1  72.1             697            721.016602\n",
            "1929    5  77.0  34.6             792            717.288391\n",
            "926     7  77.8  49.0             490            715.323181\n",
            "2349    7  81.2  58.2             753            715.147278\n",
            "149     7  83.0  63.4             736            714.777466\n",
            "...   ...   ...   ...             ...                   ...\n",
            "768     1  18.9   4.1             454            468.264648\n",
            "1822    1  19.2   0.6             494            466.495636\n",
            "1512    1  22.8  -8.1             489            463.188019\n",
            "1815    1   9.7  -6.6             475            458.994019\n",
            "1131    1   6.9 -16.1             309            452.812561\n",
            "\n",
            "[2535 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the three models against each other and determine which one is best for predictions, you should evaluate them on a common ground, typically using a separate test set that was not used during training. The comparison should be based on their performance metrics, such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), or any other relevant metric.\n",
        "\n",
        "Here's how you can approach this:\n",
        "\n",
        "1. Separate Test Set\n",
        "\n",
        "If you haven't already, split your data into training and testing sets. This is crucial for evaluating the model's performance on unseen data. If your datasets df1, df2, and df3 have not been split and have been used entirely for training, you'll need a separate dataset for testing. If you already have a test set, proceed with that.\n",
        "\n",
        "2. Evaluate Each Model\n",
        "\n",
        "Evaluate each model on the same test set. For example, if your test set includes the features 'day', 'temp', and 'dewp', you would use the relevant subset of these features for each model"
      ],
      "metadata": {
        "id": "QA3FVGUM9Wyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Evaluate model_1\n",
        "predictions_1 = model_1.predict(test_set_1[['day']])\n",
        "mae_1 = mean_absolute_error(test_set_1['NUM_COLLISIONS'], predictions_1.flatten())\n",
        "\n",
        "# Evaluate model_2\n",
        "predictions_2 = model_2.predict(test_set_2[['day', 'temp']])\n",
        "mae_2 = mean_absolute_error(test_set_2['NUM_COLLISIONS'], predictions_2.flatten())\n",
        "\n",
        "# Evaluate model_3\n",
        "predictions_3 = model_3.predict(test_set_3[['day', 'temp', 'dewp']])\n",
        "mae_3 = mean_absolute_error(test_set_3['NUM_COLLISIONS'], predictions_3.flatten())\n",
        "\n",
        "# Print the MAE for each model\n",
        "print(f\"MAE of Model 1: {mae_1}\")\n",
        "print(f\"MAE of Model 2: {mae_2}\")\n",
        "print(f\"MAE of Model 3: {mae_3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfcNtR9h9dMZ",
        "outputId": "0d9d716a-cc39-42f2-e270-a8ac67271265"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 5ms/step\n",
            "16/16 [==============================] - 0s 4ms/step\n",
            "16/16 [==============================] - 0s 2ms/step\n",
            "MAE of Model 1: 600.6678987587462\n",
            "MAE of Model 2: 600.6774156478617\n",
            "MAE of Model 3: 600.6540629563365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "The primary objective of this analysis was to develop regression models that could predict the number of traffic collisions on any given day in New York City, aiding the emergency services in optimizing their response strategies. Three different models were developed and compared:\n",
        "\n",
        "Model 1 (Linear Regression with 'day'): Utilized only the day of the week as the predictor variable. The predictions from this model indicated a progressive increase in the number of collisions as the week progressed, with the highest number of collisions predicted for day 7. The Mean Absolute Error (MAE) for this model was approximately 600.67.\n",
        "\n",
        "Model 2 (Linear Regression with 'day' and 'temp'): Included both the day of the week and temperature. This model provided a nuanced understanding, showing that specific combinations of days and temperatures led to higher predicted collisions. For instance, the model predicted the highest collisions on day 7 with a temperature of 89.1. The MAE for this model was slightly higher than Model 1 at approximately 600.68.\n",
        "\n",
        "Model 3 (Linear Regression with 'day', 'temp', and 'dewp'): Added dew point to the features, providing an even more detailed analysis. This model indicated that certain combinations of day, temperature, and dew point were critical in predicting higher collision numbers. For example, the highest collisions were predicted on day 7 with a temperature of 89.1 and a dew point of 72.1. The MAE for this model was slightly lower than the other two models at approximately 600.65.\n",
        "\n",
        "Key Insights and Recommendations:\n",
        "Progression Throughout the Week: All models consistently predicted an increase in collisions towards the end of the week, suggesting a need for heightened preparedness by emergency services during these days.\n",
        "\n",
        "Impact of Weather Conditions: Models 2 and 3, which included weather conditions (temperature and dew point), offered more detailed insights, although the improvement in prediction accuracy was marginal compared to Model 1. This suggests that while weather conditions do impact collision numbers, their predictive power may not be significantly stronger than the day of the week alone.\n",
        "\n",
        "Model Selection for Emergency Services: Given the marginal difference in MAEs among the three models, Model 1 (using only the day of the week) could be preferred for its simplicity and ease of interpretation. However, if the emergency services value the additional insights from weather conditions, Model 3 offers the most comprehensive analysis with a slightly better MAE.\n",
        "\n",
        "Final Note:\n",
        "While the models provide valuable insights, the predictions should be used as one of several tools in decision-making. Other factors, such as special events, traffic pattern changes, and city-wide initiatives, should also be considered in the planning process by the emergency services. Additionally, the close MAE values across all models suggest that further refinement or incorporation of additional data sources could be explored to enhance predictive accuracy."
      ],
      "metadata": {
        "id": "qKvOqet5AMbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From test inputs we have used, the first element in the array here is similar to this actual data:\n",
        "\n",
        "\"165\",1,62.4,5.6,0.668458757342322\n",
        "\n",
        "We can see that the temperature is slightly higher in the test data (which means less trips, but slightly higher wind means more trips. So, the difference between (actual) 0.668 and (predicted) 0.576 (rounded to 3 significant figures) seems reasonable.\n",
        "\n",
        "Similarly with the second:\n",
        "\n",
        "\"389\",1,26.6,3.1,0.763954173062719, which has higher number of trips due to a lower temperature and also with a slightly higher wind speed.\n",
        "\n",
        "And with the third:\n",
        "\n",
        "\"571\",1,77.2,8.4,0.724652060408235\n",
        "\n",
        "The last prediction with the higher temperature seems to punish the values more."
      ],
      "metadata": {
        "id": "MZNN3Pdbj1I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test uses day 6 (Friday) instead of day 1 (Sunday) which shows higher number of trips. The other values were left the same."
      ],
      "metadata": {
        "id": "03hutuiGkR8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to think about for the assignment. Make a validation set i.e. 5% of the data (or maybe more). This should be used for this type of testing. My values are simply made up.\n",
        "\n",
        "You should also remember to use different models with different data. In this case, I would maybe take each input valuable separately and make a regression model for each, then different variations i.e. any 2.\n",
        "\n",
        "Remember, you need to write up your results in the assignment."
      ],
      "metadata": {
        "id": "0Mq41NEMkdqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - DNN Regression Model**"
      ],
      "metadata": {
        "id": "8zhGsSgaCBcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dg0eMJQagw7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needed to create the data frame\n",
        "import pandas as pd\n",
        "\n",
        "# needed to help with speedy maths based calculations\n",
        "import numpy as np\n",
        "\n",
        "# create data frame from csv file we hosted on our github\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/22015866uhi/22015866_Data_Analytics/main/dnnregressiondata1.csv', index_col=0)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "OuwDgXbxCJJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0c5c9e-4fd3-4d4e-c8fe-ef9aa672352e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Aug  Dec  Feb  Jan  Jul  Jun  Mar  May  Nov  Oct  ...  Sun  Thu  Tue  \\\n",
            "Apr                                                    ...                  \n",
            "0      0    0    0    1    0    0    0    0    0    0  ...    0    0    1   \n",
            "0      0    0    0    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "0      0    0    0    1    0    0    0    0    0    0  ...    0    1    0   \n",
            "0      0    0    0    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "0      0    0    0    1    0    0    0    0    0    0  ...    0    0    0   \n",
            "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "0      0    1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "0      0    1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "0      0    1    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
            "0      0    1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
            "0      0    1    0    0    0    0    0    0    0    0  ...    0    0    1   \n",
            "\n",
            "     Wed  year  month  temp  prcp  dewp NUM_COLLISIONS  \n",
            "Apr                                                     \n",
            "0      0  2013    Jan  37.8  0.00  23.6            381  \n",
            "0      1  2013    Jan  27.1  0.00  10.5            480  \n",
            "0      0  2013    Jan  28.4  0.00  14.1            549  \n",
            "0      0  2013    Jan  33.4  0.00  18.6            505  \n",
            "0      0  2013    Jan  36.1  0.00  18.7            389  \n",
            "..   ...   ...    ...   ...   ...   ...            ...  \n",
            "0      0  2019    Dec  49.4  0.00  40.9            448  \n",
            "0      0  2019    Dec  48.0  0.00  37.4            355  \n",
            "0      0  2019    Dec  42.6  0.00  30.2            384  \n",
            "0      0  2019    Dec  39.4  0.56  38.3            518  \n",
            "0      0  2019    Dec  38.7  0.43  34.9            443  \n",
            "\n",
            "[2535 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index if 'day' is set as index\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# make sure we have our data by printing it out\n",
        "df[:5]\n",
        "# df #all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Bf2iUa1FiF2B",
        "outputId": "58f02d12-13a1-4e92-c3e9-c97dfdc7b18b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Apr  Aug  Dec  Feb  Jan  Jul  Jun  Mar  May  Nov  ...  Sun  Thu  Tue  Wed  \\\n",
              "0    0    0    0    0    1    0    0    0    0    0  ...    0    0    1    0   \n",
              "1    0    0    0    0    1    0    0    0    0    0  ...    0    0    0    1   \n",
              "2    0    0    0    0    1    0    0    0    0    0  ...    0    1    0    0   \n",
              "3    0    0    0    0    1    0    0    0    0    0  ...    0    0    0    0   \n",
              "4    0    0    0    0    1    0    0    0    0    0  ...    0    0    0    0   \n",
              "\n",
              "   year  month  temp  prcp  dewp  NUM_COLLISIONS  \n",
              "0  2013    Jan  37.8   0.0  23.6             381  \n",
              "1  2013    Jan  27.1   0.0  10.5             480  \n",
              "2  2013    Jan  28.4   0.0  14.1             549  \n",
              "3  2013    Jan  33.4   0.0  18.6             505  \n",
              "4  2013    Jan  36.1   0.0  18.7             389  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86ddb14b-0689-422d-927b-08653ef64aec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Apr</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Dec</th>\n",
              "      <th>Feb</th>\n",
              "      <th>Jan</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Jun</th>\n",
              "      <th>Mar</th>\n",
              "      <th>May</th>\n",
              "      <th>Nov</th>\n",
              "      <th>...</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Thu</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>temp</th>\n",
              "      <th>prcp</th>\n",
              "      <th>dewp</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jan</td>\n",
              "      <td>37.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.6</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jan</td>\n",
              "      <td>27.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jan</td>\n",
              "      <td>28.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.1</td>\n",
              "      <td>549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jan</td>\n",
              "      <td>33.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013</td>\n",
              "      <td>Jan</td>\n",
              "      <td>36.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86ddb14b-0689-422d-927b-08653ef64aec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86ddb14b-0689-422d-927b-08653ef64aec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86ddb14b-0689-422d-927b-08653ef64aec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dc31b8a4-9fd2-4e15-be93-2efca6ee33ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc31b8a4-9fd2-4e15-be93-2efca6ee33ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dc31b8a4-9fd2-4e15-be93-2efca6ee33ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_input_data = [df[\"year\"], df[\"temp\"], df[\"dewp\"], df[\"prcp\"], df[\"Sat\"], df[\"Sun\"], df[\"Mon\"], df[\"Tue\"], df[\"Wed\"], df[\"Thu\"], df[\"Fri\"], df[\"Jan\"], df[\"Feb\"], df[\"Mar\"], df[\"Apr\"], df[\"May\"], df[\"Jun\"], df[\"Jul\"], df[\"Aug\"], df[\"Sep\"], df[\"Oct\"], df[\"Nov\"], df[\"Dec\"], df[\"NUM_COLLISIONS\"]]\n",
        "headers = [\"year\",\"temp\", \"dewp\", \"prcp\", \"Sat\",\"Sun\",\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\",\"NUM_COLLISIONS\"]\n",
        "df_dnn = pd.concat(dnn_input_data, axis=1, keys=headers)\n",
        "df_dnn.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CQon4_FGi_r9",
        "outputId": "914a8138-d11e-4c02-cc8f-bbd27bb69248"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year  temp  dewp  prcp  Sat  Sun  Mon  Tue  Wed  Thu  ...  Apr  May  Jun  \\\n",
              "0  2013  37.8  23.6   0.0    0    0    0    1    0    0  ...    0    0    0   \n",
              "1  2013  27.1  10.5   0.0    0    0    0    0    1    0  ...    0    0    0   \n",
              "2  2013  28.4  14.1   0.0    0    0    0    0    0    1  ...    0    0    0   \n",
              "3  2013  33.4  18.6   0.0    0    0    0    0    0    0  ...    0    0    0   \n",
              "4  2013  36.1  18.7   0.0    1    0    0    0    0    0  ...    0    0    0   \n",
              "\n",
              "   Jul  Aug  Sep  Oct  Nov  Dec  NUM_COLLISIONS  \n",
              "0    0    0    0    0    0    0             381  \n",
              "1    0    0    0    0    0    0             480  \n",
              "2    0    0    0    0    0    0             549  \n",
              "3    0    0    0    0    0    0             505  \n",
              "4    0    0    0    0    0    0             389  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a2426e6-ba84-48d9-a565-55f2029b7360\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>temp</th>\n",
              "      <th>dewp</th>\n",
              "      <th>prcp</th>\n",
              "      <th>Sat</th>\n",
              "      <th>Sun</th>\n",
              "      <th>Mon</th>\n",
              "      <th>Tue</th>\n",
              "      <th>Wed</th>\n",
              "      <th>Thu</th>\n",
              "      <th>...</th>\n",
              "      <th>Apr</th>\n",
              "      <th>May</th>\n",
              "      <th>Jun</th>\n",
              "      <th>Jul</th>\n",
              "      <th>Aug</th>\n",
              "      <th>Sep</th>\n",
              "      <th>Oct</th>\n",
              "      <th>Nov</th>\n",
              "      <th>Dec</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2013</td>\n",
              "      <td>37.8</td>\n",
              "      <td>23.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2013</td>\n",
              "      <td>27.1</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013</td>\n",
              "      <td>28.4</td>\n",
              "      <td>14.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>33.4</td>\n",
              "      <td>18.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013</td>\n",
              "      <td>36.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a2426e6-ba84-48d9-a565-55f2029b7360')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a2426e6-ba84-48d9-a565-55f2029b7360 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a2426e6-ba84-48d9-a565-55f2029b7360');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-245e86aa-c503-44f5-ae0f-29b9ff818afc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-245e86aa-c503-44f5-ae0f-29b9ff818afc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-245e86aa-c503-44f5-ae0f-29b9ff818afc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = df_dnn.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df_dnn.drop(training_dataset.index)"
      ],
      "metadata": {
        "id": "81gx0Jkvj4Gq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features = training_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "training_labels = training_features.pop('NUM_COLLISIONS')*SCALE_NUM_COLLISIONS\n",
        "test_labels = test_features.pop('NUM_COLLISIONS')*SCALE_NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "HWi2FEuqkDFv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A scale for a constant will be useful\n",
        "SCALE_NUM_COLLISIONS = 0.001"
      ],
      "metadata": {
        "id": "yrYmVICUkYnc"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = training_labels/SCALE_NUM_COLLISIONS\n",
        "test_labels = test_labels/SCALE_NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "J8GFMtRNkwdZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features))"
      ],
      "metadata": {
        "id": "sjkPlTgClAS9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the only difference, instead of a single layer, we have our normalisation layer (22 inputs), 2 layers of 48, with 1 output. The 48 can be adjusted to improve the net.\n",
        "dnn_model_1 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_1.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "id": "PkuhkfHwlHFx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_1.fit(\n",
        "    training_features,\n",
        "    training_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyuArbGylVtD",
        "outputId": "2f5fa1ed-948d-4877-98ca-424f3a6c40ac"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.9 s, sys: 659 ms, total: 17.5 s\n",
            "Wall time: 21.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remember, we want to minimise this. The model with the lowest is the best.\n",
        "dnn_model_1_results = dnn_model_1.evaluate(test_features, test_labels, verbose=0)\n",
        "print(dnn_model_1_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z6DOhMFlrcS",
        "outputId": "eca66988-22cc-4962-927d-d533362b6f40"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54.296348571777344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure the labels match up with the dataframe from earlier.\n",
        "input_1 = pd.DataFrame.from_dict(data =\n",
        "\t\t\t\t{\n",
        "         'year' : [2018,2019,2018],\n",
        "         'temp' : [60.1, 67.5, 77.0],\n",
        "         'dewp' : [54.1, 60.9, 59.7],\n",
        "         'prcp' : [0.2,0,0.24],\n",
        "         'Sat' : [0,0,0],\n",
        "         'Sun' : [0,0,0],\n",
        "         'Mon' : [0,0,0],\n",
        "         'Tue' : [0,0,0],\n",
        "         'Wed' : [0,0,0],\n",
        "         'Thu' : [0,0,0],\n",
        "         'Fri' : [1,1,1],\n",
        "         'Jan' : [0,0,0],\n",
        "         'Feb' : [0,0,1],\n",
        "         'Mar' : [0,0,0],\n",
        "         'Apr' : [0,0,0],\n",
        "         'May' : [0,0,0],\n",
        "         'Jun' : [0,0,0],\n",
        "         'Jul' : [0,0,0],\n",
        "         'Aug' : [1,0,0],\n",
        "         'Sep' : [0,0,0],\n",
        "         'Oct' : [0,0,0],\n",
        "         'Nov' : [0,0,0],\n",
        "         'Dec' : [0,1,0],\n",
        "        })"
      ],
      "metadata": {
        "id": "Uu6Y6-F-naJt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_day_predictions = dnn_model_1.predict(input_1[:3])*SCALE_NUM_COLLISIONS\n",
        "linear_day_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn0Gx2Ylo5aB",
        "outputId": "9faa652e-9ae7-49f2-b6f3-28e7b519c62f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.58995116],\n",
              "       [0.70589536],\n",
              "       [0.64243084]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first, you can see below, we are in August, Saturday (we have a monday), 2009. We have a lower temperature and lower windspeed. The difference in day is likely to account for the higher actual number of trips.\n",
        "\n",
        "205 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2009 64.9 4.2 0.700748221591537\n",
        "\n",
        "For second (December):\n",
        "\n",
        "337 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2009 35.2 6.6 0.62902717790116\n",
        "\n",
        "For third (February):\n",
        "\n",
        "28 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2009 38.1 3.6 0.675798566550843\n",
        "\n",
        "The results overall look reasonable. You can look into this further."
      ],
      "metadata": {
        "id": "T_LUbcH6poQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerations for Assignment. I would likely use another method to either standardise or normalise num_trips. And perhaps not bother and scale them in here with SCALE_NUM_TRIPS with a generally large number.\n",
        "\n",
        "Obviously, you will likely want to have a number of variations but there is no reason you can't use most of the data given. Remember, the DNN is trying to solve a complex relationship, not a linear one.\n",
        "\n",
        "Other things to consider when doing this would be to take validation. Of course, in the assignment you will have a much larger range of data i.e. from some date x to y.\n",
        "\n",
        "This will give you more data. Taking real validation data that hasn't been shown to the model will give real results for you to check against rather than what I have done here (which is just for information)."
      ],
      "metadata": {
        "id": "4W3tvOBSpsS9"
      }
    }
  ]
}